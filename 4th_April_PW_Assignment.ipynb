{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ace7f0f6",
   "metadata": {},
   "source": [
    "## Q1. Describe the decision tree classifier algorithm and how it works to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0350893e",
   "metadata": {},
   "source": [
    "A **Decision Tree Classifier** is a supervised learning algorithm used for classification tasks. It works by splitting the data into subsets based on the values of the input features. This process is repeated recursively, resulting in a tree-like model of decisions.\n",
    "\n",
    "### How Decision Tree Classifier Works\n",
    "\n",
    "1. **Choosing the Best Split**:\n",
    "   - The algorithm starts at the root node (the entire dataset) and selects the best feature to split the data based on a criterion such as **Gini impurity**, **entropy**, or **information gain**. The best split is one that results in the most homogeneous subsets.\n",
    "\n",
    "2. **Splitting the Data**:\n",
    "   - The dataset is divided into two or more subsets (branches) based on the chosen feature. The goal is to create child nodes that are as pure as possible, meaning they contain samples from only one class as much as possible.\n",
    "\n",
    "3. **Recursive Splitting**:\n",
    "   - The splitting process is applied recursively to each child node, creating additional splits and further refining the tree. This process continues until a stopping criterion is met, such as reaching a maximum depth, having a minimum number of samples in a node, or having a pure node.\n",
    "\n",
    "4. **Leaf Nodes**:\n",
    "   - The end points of the tree are called leaf nodes, which contain the final predictions. Each leaf node represents a class label (in classification) or a continuous value (in regression). The class label is usually determined by the majority class of samples in that leaf.\n",
    "\n",
    "5. **Making Predictions**:\n",
    "   - To make a prediction, the decision tree starts at the root and traverses down the tree by following the branches based on the feature values of the input sample. The path taken ends at a leaf node, where the prediction is made.\n",
    "\n",
    "### Example of Decision Tree Classifier\n",
    "\n",
    "Consider a simple dataset with features like \"Age\" and \"Income\" to classify whether a person will buy a product (Yes or No).\n",
    "\n",
    "1. **Root Node**: The algorithm might start by splitting the data based on the \"Age\" feature. For example, it could create branches for \"Age < 30\" and \"Age ≥ 30\".\n",
    "\n",
    "2. **Child Nodes**: For the \"Age < 30\" group, it might further split based on \"Income\", creating branches for \"Income < 50K\" and \"Income ≥ 50K\".\n",
    "\n",
    "3. **Leaf Nodes**: Eventually, the algorithm reaches leaf nodes where it assigns the class label (Yes or No) based on the majority class of samples in that leaf.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **Gini Impurity**: Measures the frequency at which any element is misclassified. It ranges from 0 (pure) to 0.5 (completely impure).\n",
    "\n",
    "- **Entropy**: Measures the level of impurity or disorder in a set. It ranges from 0 (pure) to 1 (maximum disorder).\n",
    "\n",
    "- **Information Gain**: The reduction in entropy or impurity after a dataset is split on an attribute.\n",
    "\n",
    "### Advantages and Disadvantages\n",
    "\n",
    "**Advantages**:\n",
    "- Easy to understand and interpret.\n",
    "- Requires little data preprocessing (e.g., no need for normalization).\n",
    "- Can handle both numerical and categorical data.\n",
    "\n",
    "**Disadvantages**:\n",
    "- Prone to overfitting, especially with a large number of features.\n",
    "- Can be unstable; small changes in data can result in a completely different tree.\n",
    "- Not good at generalizing beyond the training data if not pruned or regularized.\n",
    "\n",
    "**Pruning** is a technique used to reduce the size of the tree and improve its generalization by removing nodes that provide little power in predicting target variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3219bf6f",
   "metadata": {},
   "source": [
    "## Q2. Provide a step-by-step explanation of the mathematical intuition behind decision tree classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed946f2",
   "metadata": {},
   "source": [
    "## Mathematical Intuition Behind Decision Tree Classification\n",
    "\n",
    "Decision trees are a supervised machine learning algorithm that mimics human decision-making processes. They break down data into smaller subsets based on specific conditions, ultimately classifying data points into different categories. The core idea is to find the best splits in the data that maximize the separation of classes.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Entropy:** Measures the impurity or randomness in a dataset. Higher entropy indicates a more mixed dataset, while lower entropy implies a purer dataset with similar class labels.\n",
    "   * Mathematically, entropy is calculated as:\n",
    "     ```\n",
    "     Entropy(S) = - Σ [p(i) * log2(p(i))]\n",
    "     ```\n",
    "     where:\n",
    "     * S is the dataset\n",
    "     * p(i) is the probability of class i in the dataset\n",
    "\n",
    "2. **Information Gain:** Measures the decrease in entropy after splitting a dataset based on a particular feature. A higher information gain indicates a better split.\n",
    "   * Mathematically, information gain is calculated as:\n",
    "     ```\n",
    "     Information Gain(S, A) = Entropy(S) - [Weighted Average of Entropy(S_v)]\n",
    "     ```\n",
    "     where:\n",
    "     * S is the dataset\n",
    "     * A is the attribute (feature) used for splitting\n",
    "     * S_v is the subset of S for which attribute A has value v\n",
    "\n",
    "3. **Gini Impurity:** Another measure of impurity, similar to entropy but computationally simpler.\n",
    "   * Mathematically, Gini impurity is calculated as:\n",
    "     ```\n",
    "     Gini Impurity(S) = 1 - Σ [p(i)^2]\n",
    "     ```\n",
    "\n",
    "### Building a Decision Tree\n",
    "\n",
    "1. **Start with the entire dataset as the root node.**\n",
    "2. **Calculate information gain (or Gini impurity) for each feature.**\n",
    "3. **Select the feature with the highest information gain (or lowest Gini impurity) as the splitting criterion.**\n",
    "4. **Split the dataset into subsets based on the chosen feature.**\n",
    "5. **Repeat steps 2-4 for each subset until a stopping criterion is met.**\n",
    "   * This stopping criterion could be a maximum depth of the tree, a minimum number of samples in a leaf node, or when the information gain becomes negligible.\n",
    "\n",
    "### Decision Making\n",
    "\n",
    "Once the decision tree is constructed, classifying new data points involves traversing the tree from the root node to a leaf node. At each internal node, the value of the corresponding feature is compared to the splitting condition to determine the next branch to follow. The final leaf node reached indicates the predicted class for the data point.\n",
    "\n",
    "### Visual Representation\n",
    "\n",
    "Decision trees are often visualized as a tree-like structure, with internal nodes representing features and branches representing possible values. Leaf nodes represent the predicted classes.\n",
    "\n",
    "### Key Points to Remember\n",
    "\n",
    "* Decision trees are easy to interpret and visualize.\n",
    "* They can handle both numerical and categorical data.\n",
    "* They are prone to overfitting, especially with deep trees.\n",
    "* Techniques like pruning and ensemble methods (Random Forest, Gradient Boosting) can help mitigate overfitting.\n",
    "\n",
    "By understanding these concepts and the mathematical calculations involved, you can gain a deeper appreciation for how decision trees work and make informed decisions when building and using them for classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01f9f0e",
   "metadata": {},
   "source": [
    "## Q3. Explain how a decision tree classifier can be used to solve a binary classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9053f4de",
   "metadata": {},
   "source": [
    "## Decision Trees for Binary Classification\n",
    "\n",
    "A decision tree is a supervised machine learning algorithm that can be effectively used for binary classification problems. It creates a model that predicts a target variable by learning simple decision rules inferred from the data features.\n",
    "\n",
    "**Steps involved in using a decision tree for binary classification:**\n",
    "\n",
    "1. **Data Preparation:**\n",
    "   * Collect and preprocess the data, ensuring it's clean and suitable for analysis.\n",
    "   * Divide the dataset into features (independent variables) and the target variable (dependent variable).\n",
    "   * For binary classification, the target variable will have two possible values, often represented as 0 and 1 or positive and negative.\n",
    "\n",
    "2. **Building the Decision Tree:**\n",
    "   * The algorithm starts at the root node, containing the entire dataset.\n",
    "   * It selects the best feature to split the data based on a metric like information gain or Gini impurity.\n",
    "   * The dataset is divided into subsets based on the chosen feature.\n",
    "   * The process is repeated recursively for each subset until a stopping criterion is met (e.g., maximum depth, minimum number of samples in a leaf node).\n",
    "\n",
    "3. **Making Predictions:**\n",
    "   * To classify a new data point, start at the root node.\n",
    "   * Follow the decision rules based on the feature values of the data point.\n",
    "   * Traverse the tree until a leaf node is reached.\n",
    "   * The class label of the leaf node is the predicted class for the new data point.\n",
    "\n",
    "**Example:**\n",
    "Imagine predicting whether an email is spam or not (binary classification).\n",
    "\n",
    "* **Features:** Length of the email, number of capital letters, presence of certain words (e.g., \"free\", \"money\"), etc.\n",
    "* **Target variable:** Spam (1) or not spam (0).\n",
    "\n",
    "The decision tree might look like this:\n",
    "\n",
    "* If the email length is greater than 100 words:\n",
    "  * If the number of capital letters is greater than 10:\n",
    "    * If the word \"free\" is present: Spam (1)\n",
    "    * Else: Not spam (0)\n",
    "  * Else: Not spam (0)\n",
    "* Else: Not spam (0)\n",
    "\n",
    "**Key points:**\n",
    "\n",
    "* Decision trees are interpretable, making it easy to understand the decision-making process.\n",
    "* They can handle both numerical and categorical data.\n",
    "* Prone to overfitting, which can be mitigated using techniques like pruning or ensemble methods.\n",
    "* Efficient for handling large datasets.\n",
    "\n",
    "By following these steps and considering the key points, you can effectively use decision trees to solve binary classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d515be6",
   "metadata": {},
   "source": [
    "## Q4. Discuss the geometric intuition behind decision tree classification and how it can be used to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7313f3d5",
   "metadata": {},
   "source": [
    "## Geometric Intuition Behind Decision Trees\n",
    "\n",
    "While decision trees are often explained in terms of information theory (entropy, information gain), they also have a geometric interpretation. This perspective provides valuable insights into how the model partitions the feature space.\n",
    "\n",
    "### Decision Boundaries as Hyperplanes\n",
    "\n",
    "* **Feature Space:** Each feature in your dataset represents a dimension. Thus, a dataset with two features can be visualized as a 2D plane, three features as a 3D space, and so on. This space is called the feature space.\n",
    "* **Decision Boundaries:** A decision tree creates a series of decision rules, each of which can be visualized as a hyperplane. A hyperplane is a flat subspace of one dimension lower than the ambient space. For instance, in 2D, it's a line; in 3D, it's a plane.\n",
    "* **Recursive Partitioning:** Each split in the decision tree corresponds to a hyperplane that divides the feature space into two regions. These regions are then further divided by subsequent splits, creating a hierarchical structure of hyperplanes.\n",
    "\n",
    "### Geometric Interpretation of Prediction\n",
    "\n",
    "When a new data point arrives, it's plotted in the feature space. The decision tree then guides the point through the series of hyperplanes, determining which region it belongs to. The final region, represented by a leaf node in the tree, determines the predicted class.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Consider a binary classification problem with two features, X1 and X2. The decision tree might create a decision boundary like this:\n",
    "\n",
    "* **First split:** X1 > 5. This divides the feature space into two half-planes.\n",
    "* **Second split in the left half-plane:** X2 < 3. This further divides the left half-plane into two quadrants.\n",
    "\n",
    "A new data point with values X1 = 4 and X2 = 2 would fall into the lower-left quadrant, and its predicted class would be determined by the class label associated with that region.\n",
    "\n",
    "### Visualizing Higher Dimensions\n",
    "\n",
    "While it's challenging to visualize hyperplanes in high-dimensional spaces, the concept remains the same. Decision trees recursively partition the feature space into smaller and smaller regions, with each partition defined by a hyperplane.\n",
    "\n",
    "### Limitations of Geometric Intuition\n",
    "\n",
    "* **Complex Decision Boundaries:** While decision trees can create complex decision boundaries, they might not be as flexible as other models like support vector machines (SVMs) for capturing intricate patterns.\n",
    "* **High-Dimensional Data:** Visualizing and understanding the geometric interpretation becomes increasingly difficult as the number of features grows.\n",
    "\n",
    "**In conclusion,** the geometric perspective provides a complementary understanding of decision trees. While information-theoretic measures guide the tree construction, visualizing the decision boundaries can offer insights into the model's behavior and limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c366963d",
   "metadata": {},
   "source": [
    "## Q5. Define the confusion matrix and describe how it can be used to evaluate the performance of a classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796f217f",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "A **confusion matrix** is a performance evaluation tool used in machine learning classification problems. It's a table that summarizes the performance of a classification algorithm by comparing the predicted labels to the true labels.\n",
    "\n",
    "**Structure of a Confusion Matrix:**\n",
    "\n",
    "A confusion matrix for a binary classification problem looks like this:\n",
    "\n",
    "| | Predicted Negative | Predicted Positive |\n",
    "|---|---|---|\n",
    "| Actual Negative | True Negative (TN) | False Positive (FP) |\n",
    "| Actual Positive | False Negative (FN) | True Positive (TP) |\n",
    "\n",
    "* **True Positive (TP):** Correctly predicted positive cases.\n",
    "* **True Negative (TN):** Correctly predicted negative cases.\n",
    "* **False Positive (FP):** Incorrectly predicted as positive (Type I error).\n",
    "* **False Negative (FN):** Incorrectly predicted as negative (Type II error).\n",
    "\n",
    "### Evaluating Model Performance using Confusion Matrix\n",
    "\n",
    "The confusion matrix provides a comprehensive overview of the model's performance. Based on the values in the matrix, various metrics can be calculated:\n",
    "\n",
    "* **Accuracy:** Overall correctness of the model.\n",
    "   * Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "* **Precision:** Ability of the model to correctly predict positive cases out of all predicted positive cases.\n",
    "   * Precision = TP / (TP + FP)\n",
    "* **Recall (Sensitivity):** Ability of the model to correctly predict positive cases out of all actual positive cases.\n",
    "   * Recall = TP / (TP + FN)\n",
    "* **F1-score:** Harmonic mean of precision and recall.\n",
    "   * F1-score = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "\n",
    "**Choosing the right metric:**\n",
    "\n",
    "The choice of metric depends on the specific problem. For example:\n",
    "\n",
    "* **Precision** is important when false positives are costly (e.g., spam filtering).\n",
    "* **Recall** is important when false negatives are costly (e.g., disease diagnosis).\n",
    "* **F1-score** is a good balance between precision and recall.\n",
    "\n",
    "By analyzing the confusion matrix and calculating these metrics, you can gain insights into the strengths and weaknesses of your classification model and make informed decisions about improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9ae35e",
   "metadata": {},
   "source": [
    "## Q6. Provide an example of a confusion matrix and explain how precision, recall, and F1 score can be calculated from it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9031d71d",
   "metadata": {},
   "source": [
    "Sure!\n",
    "\n",
    "Here is an example of a confusion matrix for a binary classification problem:\n",
    "\n",
    "| Confusion Matrix | Predicted Negative | Predicted Positive |\n",
    "|---|---|---|\n",
    "| Actual Negative | True Negative (TN) | False Positive (FP) | 20 | 5 |\n",
    "| Actual Positive | False Negative (FN) | True Positive (TP) | 3 | 12 |\n",
    "\n",
    "**How to calculate precision, recall, and F1-score from a confusion matrix:**\n",
    "\n",
    "* **True Positive (TP):** 12 (Correctly predicted positive cases)\n",
    "* **True Negative (TN):** 20 (Correctly predicted negative cases)\n",
    "* **False Positive (FP):** 5 (Incorrectly predicted as positive)\n",
    "* **False Negative (FN):** 3 (Incorrectly predicted as negative)\n",
    "\n",
    "**Accuracy:**\n",
    "\n",
    "* Overall correctness of the model.\n",
    "* Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "* Accuracy = (12 + 20) / (12 + 20 + 5 + 3) = 0.8\n",
    "\n",
    "**Precision:**\n",
    "\n",
    "* Ability of the model to correctly predict positive cases out of all predicted positive cases.\n",
    "* Precision = TP / (TP + FP)\n",
    "* Precision = 12 / (12 + 5) = 0.7059\n",
    "\n",
    "**Recall:**\n",
    "\n",
    "* Ability of the model to correctly predict positive cases out of all actual positive cases.\n",
    "* Recall = TP / (TP + FN)\n",
    "* Recall = 12 / (12 + 3) = 0.8\n",
    "\n",
    "**F1-score:**\n",
    "\n",
    "* Harmonic mean of precision and recall.\n",
    "* F1-score = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "* F1-score = 2 * (0.7059 * 0.8) / (0.7059 + 0.8) = 0.75\n",
    "\n",
    "In this example, the model has an accuracy of 80%, meaning it correctly classified 80% of the cases. The precision is 70.59%, which means that out of all the cases predicted as positive, 70.59% were actually positive. The recall is 80%, which means that out of all the actual positive cases, the model correctly identified 80%. Finally, the F1-score is 75%, which is a harmonic mean between precision and recall, providing a balanced view of the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daae4646",
   "metadata": {},
   "source": [
    "## Q7. Discuss the importance of choosing an appropriate evaluation metric for a classification problem and explain how this can be done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae4a7ab",
   "metadata": {},
   "source": [
    "## Importance of Choosing an Appropriate Evaluation Metric\n",
    "\n",
    "Selecting the right evaluation metric is crucial for assessing the performance of a classification model accurately. A poorly chosen metric can lead to misleading conclusions and suboptimal model selection.\n",
    "\n",
    "**Why is it important?**\n",
    "\n",
    "* **Understanding Model Behavior:** Different metrics highlight different aspects of model performance. For example, accuracy might seem good but might be misleading in imbalanced datasets.\n",
    "* **Comparing Models:** Using consistent metrics allows for fair comparisons between different models.\n",
    "* **Business Impact:** The choice of metric should align with the specific goals and consequences of the model's predictions.\n",
    "\n",
    "**How to Choose the Right Metric:**\n",
    "\n",
    "1. **Understand the Problem:**\n",
    "   * **Type of Classification:** Binary, multi-class, or multi-label.\n",
    "   * **Class Distribution:** Balanced or imbalanced dataset.\n",
    "   * **Cost of Errors:** The consequences of false positives and false negatives.\n",
    "\n",
    "2. **Consider Available Metrics:**\n",
    "   * **Accuracy:** Overall correct predictions.\n",
    "   * **Precision:** Proportion of positive predictions that are correct.\n",
    "   * **Recall (Sensitivity):** Proportion of actual positive cases correctly identified.\n",
    "   * **F1-score:** Harmonic mean of precision and recall.\n",
    "   * **Confusion Matrix:** Provides a detailed breakdown of predictions.\n",
    "   * **ROC curve and AUC:** Visualize the trade-off between true positive rate and false positive rate.\n",
    "\n",
    "3. **Prioritize Metrics:**\n",
    "   * **Imbalanced Datasets:** Precision, recall, F1-score, or AUC-ROC.\n",
    "   * **Cost-Sensitive Problems:** Precision or recall based on the cost of errors.\n",
    "   * **Multiple Classes:** Macro-average, micro-average, or weighted average of metrics.\n",
    "\n",
    "4. **Evaluate Multiple Metrics:**\n",
    "   * Often, no single metric is perfect. Consider using multiple metrics to get a comprehensive view of model performance.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "* **Fraud Detection:** High precision is crucial to minimize false positives (accusing innocent people of fraud), even if it means missing some fraudulent cases (false negatives).\n",
    "* **Disease Diagnosis:** High recall is essential to identify as many cases as possible, even if it leads to some false positives (unnecessary treatments).\n",
    "\n",
    "**Additional Considerations:**\n",
    "\n",
    "* **Domain Expertise:** Involve experts from the problem domain to provide insights into the importance of different error types.\n",
    "* **Experimentation:** Try different metrics and evaluate their impact on model selection and performance.\n",
    "\n",
    "By carefully considering these factors, you can choose the most appropriate evaluation metric(s) for your classification problem and make informed decisions about model development and deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fea0d8b",
   "metadata": {},
   "source": [
    "## Q8. Provide an example of a classification problem where precision is the most important metric, and explain why."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb990c5",
   "metadata": {},
   "source": [
    "## Example: Fraud Detection\n",
    "\n",
    "**Problem:** Identifying fraudulent transactions in a large dataset of credit card transactions.\n",
    "\n",
    "**Why Precision is Most Important:**\n",
    "\n",
    "In fraud detection, precision is paramount because a false positive (flagging a legitimate transaction as fraudulent) can have severe consequences for the cardholder:\n",
    "\n",
    "* **Customer Dissatisfaction:** Incorrectly declined transactions can frustrate customers and damage the bank's reputation.\n",
    "* **Increased Operational Costs:** Investigating and resolving false positives is time-consuming and expensive.\n",
    "* **Loss of Business:** Customers might switch to other banks due to inconvenience.\n",
    "\n",
    "Therefore, it's crucial to minimize false positives. A model with high precision ensures that when a transaction is flagged as fraudulent, there's a high probability that it's actually fraudulent. While it's also important to catch as many fraudulent transactions as possible (high recall), minimizing false positives takes precedence to maintain customer satisfaction and operational efficiency.\n",
    "\n",
    "In this scenario, even if a few fraudulent transactions slip through (low recall), the consequences are generally less severe than incorrectly flagging legitimate transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c66fd8",
   "metadata": {},
   "source": [
    "## Q9. Provide an example of a classification problem where recall is the most important metric, and explain why."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ddb5c0",
   "metadata": {},
   "source": [
    "## Example: Cancer Detection\n",
    "\n",
    "**Problem:** Identifying patients with cancer from medical imaging data.\n",
    "\n",
    "**Why Recall is Most Important:**\n",
    "\n",
    "In cancer detection, a false negative (failing to detect cancer) can have catastrophic consequences for the patient. Early detection is crucial for successful treatment, and missing a cancer diagnosis can significantly reduce the patient's chances of survival.\n",
    "\n",
    "Therefore, maximizing recall is the primary objective. This means identifying as many cancer cases as possible, even if it leads to some false positives (incorrectly diagnosing healthy patients with cancer). False positives, while undesirable, can often be addressed with further tests or procedures. However, a false negative is irreversible.\n",
    "\n",
    "In this scenario, the potential costs of a false negative (loss of life) far outweigh the costs of a false positive (additional tests or procedures), making recall the most critical metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875f8820",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
