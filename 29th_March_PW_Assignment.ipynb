{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3e7ea34",
   "metadata": {},
   "source": [
    "## Q1. What is Lasso Regression, and how does it differ from other regression techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3da334",
   "metadata": {},
   "source": [
    "Lasso Regression, which stands for Least Absolute Shrinkage and Selection Operator, is a type of regularized regression technique that is used to improve the predictive performance and interpretability of a statistical model. Here’s a detailed explanation of Lasso Regression and how it differs from other regression techniques:\n",
    "\n",
    "### Lasso Regression\n",
    "\n",
    "**Definition**: Lasso Regression is a form of linear regression that incorporates regularization to enhance the model's performance and manage issues like multicollinearity and overfitting. It does this by adding a penalty term to the regression cost function based on the absolute values of the coefficients.\n",
    "\n",
    "**Cost Function**:\n",
    "The objective of Lasso Regression is to minimize the following cost function:\n",
    "\n",
    "\\[ \\text{Cost Function} = \\text{RSS} + \\lambda \\sum_{j=1}^{p} |\\beta_j| \\]\n",
    "\n",
    "where:\n",
    "- **RSS (Residual Sum of Squares)**: Measures the difference between the observed and predicted values.\n",
    "- **\\(\\lambda\\)**: Regularization parameter that controls the strength of the penalty.\n",
    "- **\\(\\sum_{j=1}^{p} |\\beta_j|\\)**: Sum of the absolute values of the coefficients.\n",
    "\n",
    "### Key Features of Lasso Regression\n",
    "\n",
    "1. **Feature Selection**:\n",
    "   - **Sparsity**: One of the primary features of Lasso Regression is its ability to produce sparse models. As \\(\\lambda\\) increases, Lasso can shrink some coefficients exactly to zero. This results in automatic feature selection, where only the most relevant predictors are retained in the model.\n",
    "   - **Interpretability**: By reducing the number of predictors, Lasso makes the model simpler and more interpretable, which is particularly useful in high-dimensional datasets.\n",
    "\n",
    "2. **Regularization**:\n",
    "   - **L1 Penalty**: Lasso uses an L1 norm penalty, which penalizes the absolute values of the coefficients. This L1 penalty encourages sparsity in the coefficients, making Lasso different from Ridge Regression, which uses an L2 norm penalty and does not set coefficients to zero.\n",
    "\n",
    "### Comparison with Other Regression Techniques\n",
    "\n",
    "1. **Ordinary Least Squares (OLS) Regression**:\n",
    "   - **No Regularization**: OLS regression minimizes the RSS without any penalty. It can suffer from overfitting, especially when dealing with multicollinearity or when there are more predictors than observations.\n",
    "   - **Coefficient Magnitude**: OLS does not shrink coefficients, so all predictors are included in the final model, which may result in less interpretable models if many predictors are included.\n",
    "\n",
    "2. **Ridge Regression**:\n",
    "   - **L2 Penalty**: Ridge Regression adds an L2 norm penalty to the cost function, which penalizes the square of the coefficients. Unlike Lasso, Ridge does not set coefficients to zero but instead shrinks them towards zero.\n",
    "   - **Handling Multicollinearity**: Ridge is effective in handling multicollinearity and improving the stability of the model, but it does not perform feature selection.\n",
    "\n",
    "3. **Elastic Net**:\n",
    "   - **Combination of L1 and L2 Penalties**: Elastic Net combines both L1 (from Lasso) and L2 (from Ridge) penalties. It provides a balance between Lasso’s feature selection and Ridge’s regularization.\n",
    "   - **Handling Correlated Predictors**: Elastic Net is useful when dealing with highly correlated predictors and can be a better choice when you need both feature selection and regularization.\n",
    "\n",
    "### Example\n",
    "\n",
    "Consider a dataset with predictors \\(X\\) and a response variable \\(y\\). Applying Lasso Regression would involve:\n",
    "\n",
    "1. **Model Fitting**: Fit the Lasso model to the data by minimizing the cost function, which includes both the RSS and the L1 penalty.\n",
    "2. **Feature Selection**: Analyze the coefficients obtained. Some coefficients may be exactly zero, indicating that those predictors are not included in the final model.\n",
    "3. **Interpretation**: Use the non-zero coefficients to understand the relationships between predictors and the response variable. The model is more interpretable as it includes fewer predictors.\n",
    "\n",
    "### Summary\n",
    "\n",
    "**Lasso Regression** is a regularized regression technique that adds an L1 penalty to the cost function, which encourages sparsity in the coefficients. It differs from other techniques like Ordinary Least Squares (OLS) and Ridge Regression by performing automatic feature selection and producing sparser models. Compared to OLS, Lasso helps to prevent overfitting and improve model interpretability, while compared to Ridge Regression, Lasso provides a way to perform feature selection by setting some coefficients exactly to zero. Elastic Net is another related technique that combines Lasso and Ridge penalties, offering a middle ground between feature selection and regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9717911c",
   "metadata": {},
   "source": [
    "## Q2. What is the main advantage of using Lasso Regression in feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb91254",
   "metadata": {},
   "source": [
    "The main advantage of using Lasso Regression in feature selection is its ability to **perform automatic feature selection** by setting some of the coefficients exactly to zero. This characteristic helps in simplifying the model by retaining only the most relevant predictors. Here’s a detailed explanation of this advantage:\n",
    "\n",
    "### Key Advantages of Lasso Regression for Feature Selection\n",
    "\n",
    "1. **Automatic Feature Selection**:\n",
    "   - **Sparsity**: Lasso Regression applies an L1 penalty to the cost function, which encourages sparsity in the coefficient estimates. As a result, some coefficients are shrunk exactly to zero when the regularization parameter (\\(\\lambda\\)) is sufficiently large. This means that Lasso can effectively exclude irrelevant or redundant predictors from the model.\n",
    "   - **Simplified Model**: By reducing the number of non-zero coefficients, Lasso simplifies the model. This makes the model easier to interpret and can improve its performance by focusing only on the most important features.\n",
    "\n",
    "2. **Improved Interpretability**:\n",
    "   - **Fewer Predictors**: With fewer predictors in the model, it is easier to understand and interpret the relationships between the predictors and the response variable. This can be especially valuable in domains where understanding the impact of individual predictors is crucial.\n",
    "\n",
    "3. **Reduced Overfitting**:\n",
    "   - **Regularization**: The L1 penalty in Lasso Regression not only performs feature selection but also helps to prevent overfitting by discouraging the inclusion of unnecessary predictors. This can lead to better generalization to new data, especially in high-dimensional datasets.\n",
    "\n",
    "4. **Handling High-Dimensional Data**:\n",
    "   - **Efficient in High Dimensions**: Lasso Regression is particularly useful in high-dimensional settings (where the number of predictors is large relative to the number of observations). It helps to manage and reduce the complexity of models by selecting a subset of predictors, making it feasible to work with large datasets.\n",
    "\n",
    "### Example of Lasso Regression in Feature Selection\n",
    "\n",
    "Consider a dataset with many predictors, where some are likely irrelevant to the response variable. Applying Lasso Regression would involve:\n",
    "\n",
    "1. **Fitting the Model**: Use Lasso Regression to fit the model with a chosen regularization parameter (\\(\\lambda\\)).\n",
    "2. **Analyzing Coefficients**: After fitting the model, examine the coefficients. Coefficients of some predictors may be exactly zero.\n",
    "3. **Model Interpretation**: The non-zero coefficients correspond to the predictors that are selected by Lasso. These predictors are considered important for predicting the response variable.\n",
    "\n",
    "### Example Code for Feature Selection with Lasso Regression\n",
    "\n",
    "Here’s a simple example using Python and `sklearn` to demonstrate Lasso Regression for feature selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7374e5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: [ 1  6  7  8  9 10 11 13 15 17]\n",
      "Non-zero coefficients: [15.35003212 28.34188987 39.99715227 44.85839696 10.20823838  9.54281046\n",
      " 88.8825307  51.85492263  6.02102696 40.99999221]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_regression(n_samples=100, n_features=20, noise=0.1, random_state=0)\n",
    "\n",
    "# Apply Lasso Regression\n",
    "lasso = Lasso(alpha=0.1)  # Regularization parameter\n",
    "lasso.fit(X, y)\n",
    "\n",
    "# Get coefficients\n",
    "coefficients = lasso.coef_\n",
    "\n",
    "# Identify non-zero coefficients\n",
    "selected_features = np.nonzero(coefficients)[0]\n",
    "print(\"Selected features:\", selected_features)\n",
    "print(\"Non-zero coefficients:\", coefficients[selected_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a668b0",
   "metadata": {},
   "source": [
    "In this example, the `Lasso` model is applied to a synthetic dataset, and the non-zero coefficients are identified, indicating the selected features.\n",
    "\n",
    "### Summary\n",
    "\n",
    "The main advantage of using Lasso Regression for feature selection is its ability to **automatically perform feature selection** by setting some coefficients to zero. This results in a simpler, more interpretable model with only the most relevant predictors, reducing overfitting and making the model more manageable, especially in high-dimensional data settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b533091",
   "metadata": {},
   "source": [
    "## Q3. How do you interpret the coefficients of a Lasso Regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e28d75",
   "metadata": {},
   "source": [
    "Interpreting the coefficients of a Lasso Regression model involves understanding how the regularization affects these coefficients and what they represent in the context of the model. Here’s a detailed guide to interpreting Lasso Regression coefficients:\n",
    "\n",
    "### 1. **Effect of Regularization on Coefficients**\n",
    "\n",
    "- **Shrinkage and Sparsity**: Lasso Regression applies an L1 penalty to the cost function, which encourages sparsity. As a result, some coefficients may be exactly zero. This means that Lasso effectively performs feature selection, including only the predictors with non-zero coefficients in the final model.\n",
    "- **Magnitude**: Non-zero coefficients represent the predictors that have a significant impact on the response variable. The magnitude of these coefficients reflects the strength of the relationship between the predictor and the response variable.\n",
    "\n",
    "### 2. **Coefficient Interpretation**\n",
    "\n",
    "- **Significance**: The sign of a coefficient indicates the direction of the relationship between the predictor and the response variable:\n",
    "  - **Positive Coefficient**: A positive coefficient suggests that an increase in the predictor's value is associated with an increase in the response variable.\n",
    "  - **Negative Coefficient**: A negative coefficient suggests that an increase in the predictor's value is associated with a decrease in the response variable.\n",
    "\n",
    "- **Magnitude**: The magnitude of a non-zero coefficient quantifies the expected change in the response variable for a one-unit change in the predictor, assuming all other predictors are held constant. The coefficient value indicates the strength of the predictor's influence on the response variable.\n",
    "  - **Standardized Coefficients**: If predictors are standardized (scaled to have a mean of 0 and a standard deviation of 1), the coefficients can be interpreted in terms of standard deviation units. This allows for a comparison of the relative importance of predictors.\n",
    "\n",
    "### 3. **Handling Zero Coefficients**\n",
    "\n",
    "- **Feature Selection**: Coefficients that are exactly zero indicate that the corresponding predictors are excluded from the model. Lasso’s ability to set coefficients to zero helps in feature selection by identifying and retaining only the most relevant predictors.\n",
    "- **Model Simplicity**: The presence of zero coefficients simplifies the model, making it more interpretable by focusing only on the predictors that have a non-zero impact on the response variable.\n",
    "\n",
    "### 4. **Practical Considerations**\n",
    "\n",
    "- **Regularization Parameter (\\(\\lambda\\))**: The value of the regularization parameter \\(\\lambda\\) affects the degree of shrinkage applied. A higher \\(\\lambda\\) increases the penalty, leading to more coefficients being shrunk to zero. Choosing an appropriate \\(\\lambda\\) is crucial for balancing model complexity and performance.\n",
    "- **Interpreting in Context**: Always interpret the coefficients in the context of the domain and the specific dataset. While Lasso helps with feature selection and reduces overfitting, understanding the practical significance of the coefficients requires domain knowledge.\n",
    "\n",
    "### Example\n",
    "\n",
    "Suppose you have a Lasso Regression model with the following non-zero coefficients:\n",
    "\n",
    "- **Coefficient for Predictor X1**: 2.5\n",
    "- **Coefficient for Predictor X2**: -1.2\n",
    "- **Coefficient for Predictor X3**: 0.0 (excluded from the model)\n",
    "\n",
    "**Interpretation**:\n",
    "\n",
    "- **X1**: A one-unit increase in X1 is associated with an increase of 2.5 units in the response variable, assuming other predictors are held constant.\n",
    "- **X2**: A one-unit increase in X2 is associated with a decrease of 1.2 units in the response variable, assuming other predictors are held constant.\n",
    "- **X3**: Since X3 has a coefficient of 0, it is not included in the final model. This implies that X3 does not have a significant impact on the response variable according to the Lasso model.\n",
    "\n",
    "### Example Code for Coefficient Interpretation\n",
    "\n",
    "Here’s a simple example using Python and `sklearn` to fit a Lasso Regression model and interpret the coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbd215f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-zero coefficients:\n",
      "Feature 0: 76.88100400665574\n",
      "Feature 1: 33.630317659947465\n",
      "Feature 2: 69.7909397114109\n",
      "Feature 3: 1.0176586343867147\n",
      "Feature 4: 81.68187296680989\n",
      "Feature 5: 96.22991647135635\n",
      "Feature 6: 87.80786764561866\n",
      "Feature 7: 60.88262589901148\n",
      "Feature 8: 98.89332207791637\n",
      "Feature 9: 3.1262769172868277\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_regression(n_samples=100, n_features=10, noise=0.1, random_state=0)\n",
    "\n",
    "# Apply Lasso Regression\n",
    "lasso = Lasso(alpha=0.5)  # Regularization parameter\n",
    "lasso.fit(X, y)\n",
    "\n",
    "# Get coefficients\n",
    "coefficients = lasso.coef_\n",
    "\n",
    "# Identify non-zero coefficients\n",
    "non_zero_indices = np.nonzero(coefficients)[0]\n",
    "print(\"Non-zero coefficients:\")\n",
    "for i in non_zero_indices:\n",
    "    print(f\"Feature {i}: {coefficients[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4d72b0",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "Interpreting the coefficients of a Lasso Regression model involves understanding the effects of both the magnitude and sign of the coefficients. Non-zero coefficients indicate the predictors that have a significant impact on the response variable, with their magnitude reflecting the strength of this impact. Coefficients that are exactly zero are excluded from the model, aiding in feature selection and simplifying the model. Regularization parameter \\(\\lambda\\) controls the extent of shrinkage, affecting which coefficients are zeroed out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc36a33f",
   "metadata": {},
   "source": [
    "## Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcb9b8f",
   "metadata": {},
   "source": [
    "In Lasso Regression, the primary tuning parameter you can adjust is the **regularization parameter** (\\(\\lambda\\) or sometimes denoted as **alpha** in various software packages). This parameter plays a critical role in controlling the model’s performance and characteristics. Here’s a detailed look at the tuning parameters in Lasso Regression and their effects:\n",
    "\n",
    "### 1. **Regularization Parameter (\\(\\lambda\\) or Alpha)**\n",
    "\n",
    "**Definition**: The regularization parameter controls the strength of the penalty applied to the coefficients. It determines how much the model coefficients are shrunk towards zero.\n",
    "\n",
    "**Effect on Model Performance**:\n",
    "\n",
    "- **Small \\(\\lambda\\) (Low Regularization)**:\n",
    "  - **Less Shrinkage**: With a small \\(\\lambda\\), the penalty applied to the coefficients is low, meaning that the coefficients are less shrunk towards zero. As a result, the Lasso model will include more predictors with larger coefficients.\n",
    "  - **Risk of Overfitting**: A small \\(\\lambda\\) may lead to overfitting, especially if the number of predictors is large or if there is multicollinearity. The model may capture noise rather than the underlying signal.\n",
    "\n",
    "- **Large \\(\\lambda\\) (High Regularization)**:\n",
    "  - **More Shrinkage**: With a large \\(\\lambda\\), the penalty is strong, leading to more coefficients being shrunk towards zero. This can result in some coefficients being exactly zero, effectively excluding certain predictors from the model.\n",
    "  - **Feature Selection**: A large \\(\\lambda\\) enhances feature selection by removing less relevant predictors, which can improve model interpretability and reduce overfitting. However, if \\(\\lambda\\) is too large, it may lead to underfitting, where the model is too simple and fails to capture important patterns in the data.\n",
    "\n",
    "### 2. **Other Parameters (Contextual)**\n",
    "\n",
    "While the regularization parameter is the primary tuning parameter in Lasso Regression, other parameters may be adjusted depending on the implementation or specific context:\n",
    "\n",
    "- **Normalization**:\n",
    "  - **Normalization**: Some implementations of Lasso Regression (e.g., `sklearn` in Python) include a parameter for normalization. Normalization scales the input features before applying the Lasso penalty. This ensures that the regularization applies equally to all features, regardless of their original scales. It can be important to standardize features before applying Lasso if normalization is not handled automatically.\n",
    "\n",
    "- **Solver Type**:\n",
    "  - **Solver**: Different algorithms or solvers can be used to fit the Lasso model (e.g., coordinate descent, least-angle regression). The choice of solver can impact computation time and numerical stability. However, it typically does not affect the regularization effect of \\(\\lambda\\) but rather the efficiency of model fitting.\n",
    "\n",
    "### Tuning \\(\\lambda\\) Using Cross-Validation\n",
    "\n",
    "To find the optimal value of \\(\\lambda\\), you can use cross-validation techniques:\n",
    "\n",
    "- **Grid Search**: Systematically test a range of \\(\\lambda\\) values to find the one that minimizes the cross-validation error.\n",
    "- **Random Search**: Randomly sample a set of \\(\\lambda\\) values and evaluate their performance.\n",
    "- **Cross-Validation**: Perform k-fold cross-validation to evaluate the model’s performance for different values of \\(\\lambda\\) and select the one that provides the best balance between bias and variance.\n",
    "\n",
    "### Example Code for Tuning \\(\\lambda\\) Using Grid Search\n",
    "\n",
    "Here’s a Python example using `sklearn` to tune the regularization parameter in Lasso Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cc64ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 0.004832930238571752\n",
      "Best cross-validation score: 0.9999991719580554\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_regression(n_samples=100, n_features=20, noise=0.1, random_state=0)\n",
    "\n",
    "# Define Lasso model\n",
    "lasso = Lasso()\n",
    "\n",
    "# Define parameter grid for \\(\\lambda\\) (alpha)\n",
    "param_grid = {'alpha': np.logspace(-4, 4, 20)}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(lasso, param_grid, cv=5)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Best parameter and score\n",
    "best_alpha = grid_search.best_params_['alpha']\n",
    "best_score = grid_search.best_score_\n",
    "print(f\"Best alpha: {best_alpha}\")\n",
    "print(f\"Best cross-validation score: {best_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fb67b5",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "The primary tuning parameter in Lasso Regression is the regularization parameter (\\(\\lambda\\) or alpha), which controls the strength of the penalty applied to the coefficients. Adjusting this parameter affects the model’s complexity, feature selection, and risk of overfitting or underfitting. Other parameters, such as normalization and solver type, may also be adjusted but typically affect computational aspects rather than the core regularization effect. Tuning \\(\\lambda\\) using methods like grid search or cross-validation helps find the optimal balance between model complexity and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c073468a",
   "metadata": {},
   "source": [
    "## Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef2b9e7",
   "metadata": {},
   "source": [
    "Yes, Lasso Regression can be extended to non-linear regression problems by incorporating non-linear transformations of the predictors. While Lasso Regression itself is inherently a linear model, you can use it in conjunction with feature engineering techniques to handle non-linear relationships. Here’s how you can apply Lasso Regression to non-linear regression problems:\n",
    "\n",
    "### 1. **Feature Engineering for Non-Linearity**\n",
    "\n",
    "To apply Lasso Regression to non-linear problems, you need to transform the predictors to capture non-linear relationships. This involves creating new features that represent non-linear interactions or transformations of the original predictors. Some common approaches include:\n",
    "\n",
    "- **Polynomial Features**:\n",
    "  - **Definition**: Create polynomial terms (e.g., \\(x^2\\), \\(x^3\\), etc.) from the original predictors to capture quadratic or higher-order relationships.\n",
    "  - **Implementation**: Use polynomial features to generate these terms and include them in the Lasso Regression model.\n",
    "  - **Example**: If you have a predictor \\(x\\), you can include \\(x^2\\), \\(x^3\\), etc., as additional features.\n",
    "\n",
    "- **Interaction Terms**:\n",
    "  - **Definition**: Create features that represent interactions between predictors (e.g., \\(x_1 \\times x_2\\)).\n",
    "  - **Implementation**: Include these interaction terms as additional features in the model.\n",
    "\n",
    "- **Basis Functions**:\n",
    "  - **Definition**: Use basis functions (e.g., radial basis functions, splines) to capture non-linear relationships.\n",
    "  - **Implementation**: Apply basis functions to the predictors to transform them into a form suitable for linear regression.\n",
    "\n",
    "### 2. **Using Polynomial Features in Python**\n",
    "\n",
    "Here’s how you can use polynomial features with Lasso Regression in Python using the `PolynomialFeatures` class from `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "043e9bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 25.736703882693327\n",
      "Coefficients: [0.         0.71188437 2.91899532]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(100, 1) * 10\n",
    "y = 3 * X.squeeze() ** 2 + np.random.randn(100) * 5  # Non-linear relationship\n",
    "\n",
    "# Create polynomial features\n",
    "poly = PolynomialFeatures(degree=2)  # Degree of the polynomial\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Apply Lasso Regression\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = lasso.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Coefficients\n",
    "print(\"Coefficients:\", lasso.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6bf87f",
   "metadata": {},
   "source": [
    "### 3. **Other Non-Linear Transformations**\n",
    "\n",
    "- **Log Transformations**: Apply log transformations to predictors if there is an exponential relationship.\n",
    "- **Interaction Terms**: Manually create interaction terms to capture non-linear relationships between predictors.\n",
    "\n",
    "### 4. **Considerations**\n",
    "\n",
    "- **Model Complexity**: Adding polynomial or interaction terms increases the complexity of the model. Be mindful of the risk of overfitting, especially with higher-degree polynomials or many interaction terms.\n",
    "- **Feature Scaling**: Non-linear transformations can result in features with different scales. Standardizing or normalizing the features before applying Lasso Regression is important to ensure the regularization applies equally.\n",
    "\n",
    "### Summary\n",
    "\n",
    "Lasso Regression can be applied to non-linear regression problems by transforming the predictors to capture non-linear relationships. This involves creating polynomial features, interaction terms, or using basis functions. The Lasso model then applies regularization to these transformed features, helping with feature selection and preventing overfitting. It’s important to handle the increased complexity and ensure proper scaling of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cac83a",
   "metadata": {},
   "source": [
    "## Q6. What is the difference between Ridge Regression and Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158d073b",
   "metadata": {},
   "source": [
    "Ridge Regression and Lasso Regression are both regularization techniques used to address issues like overfitting and multicollinearity in linear regression models. While they share some similarities, they differ in the way they apply regularization and their effects on the model. Here’s a detailed comparison of Ridge Regression and Lasso Regression:\n",
    "\n",
    "### 1. **Regularization Technique**\n",
    "\n",
    "- **Ridge Regression (L2 Regularization)**:\n",
    "  - **Penalty Term**: Adds an L2 penalty to the cost function, which is the sum of the squares of the coefficients.\n",
    "  - **Cost Function**:\n",
    "    \\[ \\text{Cost Function} = \\text{RSS} + \\lambda \\sum_{j=1}^{p} \\beta_j^2 \\]\n",
    "    where \\(\\lambda\\) is the regularization parameter, and \\(\\beta_j\\) are the model coefficients.\n",
    "  - **Effect on Coefficients**: Shrinks the coefficients towards zero but does not set them exactly to zero. All predictors are included in the model, but their impact is reduced.\n",
    "\n",
    "- **Lasso Regression (L1 Regularization)**:\n",
    "  - **Penalty Term**: Adds an L1 penalty to the cost function, which is the sum of the absolute values of the coefficients.\n",
    "  - **Cost Function**:\n",
    "    \\[ \\text{Cost Function} = \\text{RSS} + \\lambda \\sum_{j=1}^{p} |\\beta_j| \\]\n",
    "    where \\(\\lambda\\) is the regularization parameter, and \\(\\beta_j\\) are the model coefficients.\n",
    "  - **Effect on Coefficients**: Shrinks some coefficients exactly to zero, performing automatic feature selection. This results in a sparser model with fewer predictors.\n",
    "\n",
    "### 2. **Impact on Model Coefficients**\n",
    "\n",
    "- **Ridge Regression**:\n",
    "  - **Coefficient Shrinkage**: Reduces the magnitude of coefficients but keeps all features in the model.\n",
    "  - **No Zero Coefficients**: Coefficients are rarely exactly zero, meaning all predictors remain in the model.\n",
    "  - **Application**: Useful when all predictors are believed to be relevant but need to be regularized to reduce overfitting.\n",
    "\n",
    "- **Lasso Regression**:\n",
    "  - **Coefficient Shrinkage and Selection**: Can reduce some coefficients to exactly zero, effectively excluding certain predictors from the model.\n",
    "  - **Feature Selection**: Automatically selects a subset of predictors, which can lead to a simpler, more interpretable model.\n",
    "  - **Application**: Ideal when you suspect that only a subset of predictors is relevant and want to perform feature selection.\n",
    "\n",
    "### 3. **Handling Multicollinearity**\n",
    "\n",
    "- **Ridge Regression**:\n",
    "  - **Effective**: Regularization helps stabilize estimates in the presence of multicollinearity by shrinking the coefficients of correlated predictors.\n",
    "  - **All Predictors**: Retains all predictors, even if they are highly correlated.\n",
    "\n",
    "- **Lasso Regression**:\n",
    "  - **Effective**: Also helps with multicollinearity but through feature selection. By setting some coefficients to zero, it can effectively handle correlated predictors by selecting among them.\n",
    "  - **Feature Reduction**: May choose only one predictor from a group of highly correlated predictors.\n",
    "\n",
    "### 4. **Model Complexity**\n",
    "\n",
    "- **Ridge Regression**:\n",
    "  - **Complexity**: Tends to produce a more complex model with more predictors, as it does not perform feature selection.\n",
    "  - **Regularization Parameter (\\(\\lambda\\))**: Controls the degree of shrinkage. A higher \\(\\lambda\\) results in more shrinkage but still includes all features.\n",
    "\n",
    "- **Lasso Regression**:\n",
    "  - **Complexity**: Produces a sparser model with fewer predictors. The model complexity is reduced as some predictors are excluded.\n",
    "  - **Regularization Parameter (\\(\\lambda\\))**: Controls both the degree of shrinkage and the amount of feature selection. A higher \\(\\lambda\\) increases sparsity by setting more coefficients to zero.\n",
    "\n",
    "### 5. **Computational Considerations**\n",
    "\n",
    "- **Ridge Regression**:\n",
    "  - **Computational Efficiency**: Generally more straightforward to compute since the L2 penalty results in a smooth, differentiable cost function.\n",
    "  \n",
    "- **Lasso Regression**:\n",
    "  - **Computational Complexity**: The L1 penalty leads to a non-smooth cost function, which can make optimization more complex. Advanced algorithms are often used to handle Lasso’s optimization.\n",
    "\n",
    "### Summary\n",
    "\n",
    "**Ridge Regression** and **Lasso Regression** are both regularization techniques for linear regression but differ primarily in their penalty terms. Ridge Regression applies an L2 penalty, shrinking coefficients but retaining all predictors in the model. It’s effective for handling multicollinearity and regularizing all predictors. Lasso Regression applies an L1 penalty, which can shrink some coefficients to zero, effectively performing feature selection and simplifying the model. The choice between Ridge and Lasso depends on whether you need feature selection (Lasso) or just regularization without feature exclusion (Ridge). Elastic Net combines both L1 and L2 penalties, providing a balance between Ridge and Lasso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c474629",
   "metadata": {},
   "source": [
    "## Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e839632",
   "metadata": {},
   "source": [
    "Lasso Regression can handle multicollinearity in input features, but it does so in a specific way that differs from other regularization methods like Ridge Regression. Here’s how Lasso Regression deals with multicollinearity:\n",
    "\n",
    "### 1. **Feature Selection**\n",
    "\n",
    "- **Automatic Feature Selection**: One of the key strengths of Lasso Regression is its ability to perform automatic feature selection by shrinking some coefficients exactly to zero. This feature selection property is particularly useful in the presence of multicollinearity, where multiple predictors are highly correlated.\n",
    "- **Handling Correlated Predictors**: When predictors are highly correlated, Lasso Regression tends to select one of the correlated features and set the coefficients of the others to zero. This means that among a group of highly correlated predictors, Lasso will keep only one (or a few) and exclude the rest, effectively reducing multicollinearity.\n",
    "\n",
    "### 2. **Effectiveness in Multicollinearity**\n",
    "\n",
    "- **Reduces Complexity**: By setting some coefficients to zero, Lasso Regression simplifies the model and reduces the number of predictors. This reduction in model complexity helps in mitigating issues associated with multicollinearity because it eliminates redundant predictors.\n",
    "- **Model Interpretability**: With fewer predictors included in the model, the interpretability is improved. This is valuable in scenarios where understanding the influence of each feature is important.\n",
    "\n",
    "### 3. **Comparison with Ridge Regression**\n",
    "\n",
    "- **Ridge Regression**:\n",
    "  - **Regularization Approach**: Ridge Regression applies L2 regularization, which shrinks all coefficients towards zero but does not set any coefficients exactly to zero. It effectively reduces the impact of correlated predictors but retains all of them in the model.\n",
    "  - **Handling Multicollinearity**: Ridge Regression handles multicollinearity by reducing the magnitude of coefficients, thus stabilizing the estimates. However, it does not perform feature selection.\n",
    "\n",
    "- **Lasso Regression**:\n",
    "  - **Regularization Approach**: Lasso Regression applies L1 regularization, which can shrink some coefficients to zero. This feature selection capability is particularly useful when dealing with multicollinearity as it helps to identify and retain the most relevant predictors while excluding redundant ones.\n",
    "\n",
    "### 4. **Example of Lasso Handling Multicollinearity**\n",
    "\n",
    "Consider a dataset with highly correlated features. Here’s how Lasso Regression handles this situation:\n",
    "\n",
    "- **Data**: Assume you have a dataset with predictors \\(X_1\\), \\(X_2\\), and \\(X_3\\), where \\(X_1\\) and \\(X_2\\) are highly correlated, and \\(X_3\\) is less correlated with the others.\n",
    "\n",
    "- **Lasso Application**:\n",
    "  - **Fit Lasso Model**: When fitting a Lasso model to this data, Lasso might assign non-zero coefficients to \\(X_1\\) and set the coefficient for \\(X_2\\) to zero (or vice versa). This means \\(X_2\\) would be excluded from the model, reducing multicollinearity by eliminating one of the correlated predictors.\n",
    "  - **Interpretation**: The model might retain \\(X_1\\) and \\(X_3\\), simplifying the model and reducing redundancy due to correlation between \\(X_1\\) and \\(X_2\\).\n",
    "\n",
    "### Example Code for Lasso Handling Multicollinearity\n",
    "\n",
    "Here’s a Python example showing how Lasso Regression can be used to handle multicollinearity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "828373a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso coefficients: [38.93255103 -0.         65.08439116]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Generate synthetic data with multicollinearity\n",
    "np.random.seed(0)\n",
    "X, y = make_regression(n_samples=100, n_features=3, noise=0.1, random_state=0)\n",
    "X[:, 1] = X[:, 0] + np.random.normal(scale=0.1, size=X[:, 0].shape)  # Introduce multicollinearity\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply Lasso Regression\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X_scaled, y)\n",
    "\n",
    "# Coefficients\n",
    "print(\"Lasso coefficients:\", lasso.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861e5565",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "Lasso Regression can handle multicollinearity by performing automatic feature selection, which effectively reduces the number of predictors in the model. This is achieved by setting some coefficients to zero, thereby excluding redundant or highly correlated features. While Ridge Regression stabilizes estimates in the presence of multicollinearity by shrinking coefficients, it does not perform feature selection. Therefore, Lasso Regression is particularly useful when you want to simplify the model and address multicollinearity by excluding less relevant predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dbfb59",
   "metadata": {},
   "source": [
    "## Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ca04ce",
   "metadata": {},
   "source": [
    "Choosing the optimal value of the regularization parameter (\\(\\lambda\\) or alpha) in Lasso Regression is crucial for balancing model complexity and performance. The regularization parameter controls the strength of the penalty applied to the coefficients, influencing both the magnitude of the coefficients and the feature selection process. Here’s a detailed approach to selecting the optimal \\(\\lambda\\):\n",
    "\n",
    "### 1. **Cross-Validation**\n",
    "\n",
    "Cross-validation is the most commonly used technique to select the optimal \\(\\lambda\\). It involves partitioning the data into training and validation sets multiple times to evaluate model performance across different values of \\(\\lambda\\). Here’s how you can do it:\n",
    "\n",
    "- **Grid Search**: Perform a grid search over a range of \\(\\lambda\\) values and evaluate the model’s performance using cross-validation.\n",
    "- **Random Search**: Randomly sample a set of \\(\\lambda\\) values within a specified range and assess the performance.\n",
    "\n",
    "### 2. **Steps for Cross-Validation with Grid Search**\n",
    "\n",
    "1. **Define the Range of \\(\\lambda\\)**:\n",
    "   - Choose a range of \\(\\lambda\\) values to test. You can use a logarithmic scale to cover a wide range, as \\(\\lambda\\) values often span several orders of magnitude.\n",
    "\n",
    "2. **Split the Data**:\n",
    "   - Divide the dataset into training and validation folds. Common choices are k-fold cross-validation, where the data is split into k subsets, and Leave-One-Out Cross-Validation (LOOCV), where each data point is used once as a validation set.\n",
    "\n",
    "3. **Train and Evaluate**:\n",
    "   - For each \\(\\lambda\\) value, train the Lasso model on the training folds and evaluate its performance on the validation folds. Metrics such as Mean Squared Error (MSE) or Mean Absolute Error (MAE) are commonly used to assess performance.\n",
    "\n",
    "4. **Select the Optimal \\(\\lambda\\)**:\n",
    "   - Choose the \\(\\lambda\\) that minimizes the cross-validation error. This \\(\\lambda\\) balances the trade-off between model complexity (penalizing large coefficients) and goodness of fit.\n",
    "\n",
    "### 3. **Example Code Using Grid Search and Cross-Validation**\n",
    "\n",
    "Here’s a Python example using `sklearn` to find the optimal \\(\\lambda\\) with Grid Search and Cross-Validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "167aad39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 0.0001\n",
      "Best cross-validation score (MSE): 0.009223859029348816\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_regression(n_samples=100, n_features=10, noise=0.1, random_state=0)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Define Lasso model\n",
    "lasso = Lasso()\n",
    "\n",
    "# Define parameter grid for \\(\\lambda\\) (alpha)\n",
    "param_grid = {'alpha': np.logspace(-4, 4, 20)}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(lasso, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_scaled, y)\n",
    "\n",
    "# Best parameter and score\n",
    "best_alpha = grid_search.best_params_['alpha']\n",
    "best_score = -grid_search.best_score_  # Convert back from negative MSE\n",
    "print(f\"Best alpha: {best_alpha}\")\n",
    "print(f\"Best cross-validation score (MSE): {best_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88848d0b",
   "metadata": {},
   "source": [
    "### 4. **Alternative Methods**\n",
    "\n",
    "- **Lasso Path Algorithms**: Algorithms like `LassoCV` in `sklearn` perform Lasso Regression with built-in cross-validation to select the optimal \\(\\lambda\\). This is an efficient method for finding \\(\\lambda\\) without needing an explicit grid search.\n",
    "\n",
    "- **Information Criteria**: Although less common for Lasso, some methods use information criteria like AIC or BIC to select \\(\\lambda\\). These criteria balance model fit and complexity.\n",
    "\n",
    "### 5. **Considerations**\n",
    "\n",
    "- **Range of \\(\\lambda\\)**: Ensure the range of \\(\\lambda\\) values tested covers both very small and very large values to find an optimal balance.\n",
    "- **Model Stability**: Verify that the selected \\(\\lambda\\) provides a stable model with consistent performance across different folds of cross-validation.\n",
    "- **Interpretability**: If feature selection is crucial, ensure that the chosen \\(\\lambda\\) leads to a model with an appropriate number of non-zero coefficients.\n",
    "\n",
    "### Summary\n",
    "\n",
    "To choose the optimal value of the regularization parameter (\\(\\lambda\\)) in Lasso Regression, use cross-validation techniques such as grid search or random search over a range of \\(\\lambda\\) values. The goal is to find the \\(\\lambda\\) that minimizes cross-validation error, balancing model complexity and performance. Tools like `GridSearchCV` and `LassoCV` in Python can streamline this process, helping you select the best \\(\\lambda\\) for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86abcbba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
