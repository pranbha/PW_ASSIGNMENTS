{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2436a66",
   "metadata": {},
   "source": [
    "### Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a510bd7c",
   "metadata": {},
   "source": [
    "Min-Max scaling is a technique used in data preprocessing to scale numerical features to a fixed range, typically between 0 and 1. It works by subtracting the minimum value of the feature and then dividing by the range (the maximum value minus the minimum value).\n",
    "\n",
    "Here's the formula for Min-Max scaling:\n",
    "\n",
    "\\[ X_{\\text{scaled}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}} \\]\n",
    "\n",
    "where:\n",
    "- \\( X \\) is the original feature value.\n",
    "- \\( X_{\\text{min}} \\) is the minimum value of the feature.\n",
    "- \\( X_{\\text{max}} \\) is the maximum value of the feature.\n",
    "- \\( X_{\\text{scaled}} \\) is the scaled feature value.\n",
    "\n",
    "Min-Max scaling is beneficial when the features have different scales, and you want to bring them to a comparable range without distorting the data distribution.\n",
    "\n",
    "Here's an example to illustrate its application:\n",
    "\n",
    "Suppose you have a dataset containing a feature representing house prices (\\$) and another feature representing the size of the houses (in square feet). The house prices range from \\$100,000 to \\$1,000,000, and the house sizes range from 800 sq. ft. to 4000 sq. ft. You want to scale these features using Min-Max scaling.\n",
    "\n",
    "House Prices (\\$):\n",
    "- Minimum price (\\( X_{\\text{min}} \\)): \\$100,000\n",
    "- Maximum price (\\( X_{\\text{max}} \\)): \\$1,000,000\n",
    "\n",
    "House Sizes (sq. ft.):\n",
    "- Minimum size (\\( X_{\\text{min}} \\)): 800 sq. ft.\n",
    "- Maximum size (\\( X_{\\text{max}} \\)): 4000 sq. ft.\n",
    "\n",
    "Now, let's scale a house price of \\$500,000 and a house size of 2500 sq. ft. using Min-Max scaling:\n",
    "\n",
    "For House Price (\\$):\n",
    "\\[ X_{\\text{scaled}} = \\frac{500,000 - 100,000}{1,000,000 - 100,000} = 0.375 \\]\n",
    "\n",
    "For House Size (sq. ft.):\n",
    "\\[ X_{\\text{scaled}} = \\frac{2,500 - 800}{4,000 - 800} = 0.5 \\]\n",
    "\n",
    "After Min-Max scaling, the house price of \\$500,000 is scaled to 0.375, and the house size of 2500 sq. ft. is scaled to 0.5. Both features are now within the range [0, 1], making them directly comparable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6c8f9a",
   "metadata": {},
   "source": [
    "### Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc9609e",
   "metadata": {},
   "source": [
    "The Unit Vector technique, also known as Unit Norm scaling or Vector normalization, is a feature scaling method that scales each feature vector to have a length of 1, while preserving the direction of the vector. This technique is particularly useful when the direction of the data points is more important than their magnitudes.\n",
    "\n",
    "In Unit Vector scaling, each feature vector is divided by its Euclidean norm (also known as the L2 norm). Here's the formula:\n",
    "\n",
    "\\[ X_{\\text{scaled}} = \\frac{X}{\\|X\\|_2} \\]\n",
    "\n",
    "where:\n",
    "- \\( X \\) is the original feature vector.\n",
    "- \\( X_{\\text{scaled}} \\) is the scaled feature vector.\n",
    "- \\( \\|X\\|_2 \\) is the Euclidean norm of the feature vector.\n",
    "\n",
    "Unit Vector scaling differs from Min-Max scaling in that it doesn't necessarily bring the values of the features to a fixed range like [0, 1]. Instead, it ensures that the length (magnitude) of each feature vector becomes 1.\n",
    "\n",
    "Here's an example to illustrate its application:\n",
    "\n",
    "Suppose you have a dataset with two features: height (in inches) and weight (in pounds). You want to scale these features using Unit Vector scaling.\n",
    "\n",
    "Let's consider a data point with height = 60 inches and weight = 120 pounds.\n",
    "\n",
    "The original feature vector for this data point is \\( X = [60, 120] \\).\n",
    "\n",
    "The Euclidean norm of this vector is:\n",
    "\n",
    "\\[ \\|X\\|_2 = \\sqrt{60^2 + 120^2} \\approx 134.54 \\]\n",
    "\n",
    "Now, to scale the feature vector using Unit Vector scaling:\n",
    "\n",
    "\\[ X_{\\text{scaled}} = \\frac{[60, 120]}{134.54} \\approx \\left[\\frac{60}{134.54}, \\frac{120}{134.54}\\right] \\approx [0.446, 0.893] \\]\n",
    "\n",
    "After Unit Vector scaling, the feature vector becomes approximately [0.446, 0.893], with a length of 1. The direction of the original feature vector is preserved, but its magnitude is normalized to 1. This is useful when the relative proportions of the features are more important than their absolute values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eec0b6f",
   "metadata": {},
   "source": [
    "### Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed8f0f8",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is a popular technique used for dimensionality reduction in data analysis and machine learning. It works by transforming the original features of a dataset into a new set of orthogonal (uncorrelated) features, called principal components, which are linear combinations of the original features. These principal components are ordered in such a way that the first principal component captures the maximum variance in the data, the second principal component captures the second maximum variance, and so on.\n",
    "\n",
    "PCA is used for dimensionality reduction by retaining only the top \\( k \\) principal components that explain most of the variance in the data while discarding the rest. This reduces the dimensionality of the dataset while preserving as much of the essential information as possible.\n",
    "\n",
    "Here's how PCA is typically applied:\n",
    "\n",
    "1. **Standardization**: Standardize the features (subtract the mean and divide by the standard deviation) to ensure that each feature has a mean of 0 and a standard deviation of 1. This step is important to give all features equal importance during PCA.\n",
    "\n",
    "2. **Compute Covariance Matrix**: Calculate the covariance matrix of the standardized feature matrix.\n",
    "\n",
    "3. **Eigenvalue Decomposition**: Perform eigenvalue decomposition on the covariance matrix to obtain the eigenvectors and eigenvalues.\n",
    "\n",
    "4. **Select Principal Components**: Sort the eigenvectors based on their corresponding eigenvalues in descending order. The eigenvectors with the highest eigenvalues represent the principal components.\n",
    "\n",
    "5. **Projection**: Project the original data onto the subspace spanned by the selected principal components to obtain the reduced-dimensional representation of the data.\n",
    "\n",
    "Here's an example to illustrate PCA's application:\n",
    "\n",
    "Suppose you have a dataset containing information about houses, including features such as size (in square feet), number of bedrooms, number of bathrooms, and price. You want to reduce the dimensionality of this dataset using PCA.\n",
    "\n",
    "1. **Standardization**: Standardize the features by subtracting the mean and dividing by the standard deviation.\n",
    "\n",
    "2. **Compute Covariance Matrix**: Calculate the covariance matrix of the standardized feature matrix.\n",
    "\n",
    "3. **Eigenvalue Decomposition**: Perform eigenvalue decomposition on the covariance matrix to obtain the eigenvectors and eigenvalues.\n",
    "\n",
    "4. **Select Principal Components**: Sort the eigenvectors based on their corresponding eigenvalues. Let's say the top two eigenvectors have the highest eigenvalues, representing the first and second principal components.\n",
    "\n",
    "5. **Projection**: Project the original data onto the subspace spanned by the top two principal components to obtain the reduced-dimensional representation of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fac5316",
   "metadata": {},
   "source": [
    "### Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b395a0",
   "metadata": {},
   "source": [
    "PCA and feature extraction are closely related concepts. Feature extraction refers to the process of transforming the original features of a dataset into a new set of features, typically with reduced dimensionality, while still preserving as much relevant information as possible. PCA is a specific technique for feature extraction that aims to find the most informative linear combinations of the original features.\n",
    "\n",
    "Here's how PCA can be used for feature extraction:\n",
    "\n",
    "1. **Dimensionality Reduction**: PCA can be used to reduce the dimensionality of a dataset by transforming the original features into a smaller set of orthogonal features, called principal components. These principal components are linear combinations of the original features and capture most of the variance in the data.\n",
    "\n",
    "2. **Information Retention**: PCA retains the most important information in the dataset by selecting the principal components that explain the most variance. By discarding the principal components with lower variance, PCA effectively reduces the dimensionality of the data while preserving as much relevant information as possible.\n",
    "\n",
    "3. **Feature Transformation**: PCA transforms the original features into a new set of features represented by the principal components. These new features are uncorrelated and ordered by their importance in explaining the variance in the data.\n",
    "\n",
    "4. **Improved Model Performance**: By reducing the dimensionality of the dataset and removing redundant or noisy features, PCA can lead to improved model performance in tasks such as classification, regression, or clustering.\n",
    "\n",
    "Here's an example to illustrate how PCA can be used for feature extraction:\n",
    "\n",
    "Suppose you have a dataset containing images of handwritten digits (e.g., from 0 to 9) represented as 28x28 pixel images. Each image is flattened into a vector of 784 features (28x28 = 784). You want to extract a smaller set of features that captures the most important information in the images while reducing the dimensionality of the dataset.\n",
    "\n",
    "You can use PCA for feature extraction in the following steps:\n",
    "\n",
    "1. **Standardization**: Standardize the pixel values of the images to have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "2. **PCA**: Apply PCA to the standardized dataset to extract the principal components. These principal components represent the most informative combinations of pixel values in the images.\n",
    "\n",
    "3. **Dimensionality Reduction**: Select the top \\( k \\) principal components that capture most of the variance in the dataset. By choosing \\( k \\) to be much smaller than the original number of features (784 in this case), you achieve dimensionality reduction.\n",
    "\n",
    "4. **Feature Transformation**: Project the original images onto the subspace spanned by the selected principal components. This transforms each image into a new set of features represented by the principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3939b5a2",
   "metadata": {},
   "source": [
    "### Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bfdf1e",
   "metadata": {},
   "source": [
    "In the context of building a recommendation system for a food delivery service, Min-Max scaling can be used to preprocess the data before feeding it into the recommendation algorithm. Here's how you could apply Min-Max scaling to the features such as price, rating, and delivery time:\n",
    "\n",
    "1. **Identify Features**: First, identify the features in your dataset that need to be scaled. In this case, the features are price, rating, and delivery time.\n",
    "\n",
    "2. **Compute Min and Max Values**: Calculate the minimum and maximum values for each feature in the dataset. For example:\n",
    "   - Minimum price (\\( X_{\\text{min, price}} \\))\n",
    "   - Maximum price (\\( X_{\\text{max, price}} \\))\n",
    "   - Minimum rating (\\( X_{\\text{min, rating}} \\))\n",
    "   - Maximum rating (\\( X_{\\text{max, rating}} \\))\n",
    "   - Minimum delivery time (\\( X_{\\text{min, delivery}} \\))\n",
    "   - Maximum delivery time (\\( X_{\\text{max, delivery}} \\))\n",
    "\n",
    "3. **Apply Min-Max Scaling**: Use the Min-Max scaling formula to scale each feature to a range between 0 and 1. The formula for Min-Max scaling is:\n",
    "\n",
    "\\[ X_{\\text{scaled}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}} \\]\n",
    "\n",
    "where \\( X \\) is the original feature value, \\( X_{\\text{min}} \\) is the minimum value of the feature, and \\( X_{\\text{max}} \\) is the maximum value of the feature.\n",
    "\n",
    "4. **Apply Min-Max Scaling to Each Feature**: Scale each feature separately using its corresponding minimum and maximum values. For example:\n",
    "   - Scale the price feature: \\( X_{\\text{scaled, price}} = \\frac{\\text{price} - X_{\\text{min, price}}}{X_{\\text{max, price}} - X_{\\text{min, price}}} \\)\n",
    "   - Scale the rating feature: \\( X_{\\text{scaled, rating}} = \\frac{\\text{rating} - X_{\\text{min, rating}}}{X_{\\text{max, rating}} - X_{\\text{min, rating}}} \\)\n",
    "   - Scale the delivery time feature: \\( X_{\\text{scaled, delivery}} = \\frac{\\text{delivery time} - X_{\\text{min, delivery}}}{X_{\\text{max, delivery}} - X_{\\text{min, delivery}}} \\)\n",
    "\n",
    "5. **Use Scaled Features in Recommendation Algorithm**: Once all the features have been scaled, use the scaled features as input to your recommendation algorithm. This ensures that all features are on the same scale and have a similar influence on the recommendation process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b1cc0f",
   "metadata": {},
   "source": [
    "### Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11fc8e7",
   "metadata": {},
   "source": [
    "In the context of building a model to predict stock prices, PCA (Principal Component Analysis) can be a valuable technique to reduce the dimensionality of the dataset while retaining the most important information. Here's how you could use PCA to achieve dimensionality reduction for such a project:\n",
    "\n",
    "1. **Feature Selection**: Start by identifying the features in your dataset. These features could include company-specific financial data such as revenue, earnings, debt-to-equity ratio, etc., as well as market trends data such as sector performance, interest rates, inflation rates, etc.\n",
    "\n",
    "2. **Standardization**: Before applying PCA, it's essential to standardize the features to ensure that each feature contributes equally to the analysis. Standardization involves subtracting the mean and dividing by the standard deviation for each feature.\n",
    "\n",
    "3. **PCA Application**: Apply PCA to the standardized feature matrix. PCA will transform the original features into a set of linearly uncorrelated features called principal components. These principal components are ordered by the amount of variance they explain in the data.\n",
    "\n",
    "4. **Selecting the Number of Components**: Decide on the number of principal components to retain based on the amount of variance you want to preserve in the data. A common approach is to select the number of components that collectively explain a significant portion of the total variance in the dataset, e.g., 95%.\n",
    "\n",
    "5. **Dimensionality Reduction**: Project the original feature matrix onto the subspace spanned by the selected principal components. This effectively reduces the dimensionality of the dataset while retaining most of the relevant information.\n",
    "\n",
    "6. **Model Training**: Use the reduced-dimensional feature matrix as input to your stock price prediction model. You can employ various machine learning algorithms such as regression, time series forecasting models, or neural networks to train your model.\n",
    "\n",
    "Benefits of using PCA for dimensionality reduction in this context include:\n",
    "- Simplification of the model: With fewer features, the model becomes less complex and easier to interpret.\n",
    "- Removal of multicollinearity: PCA eliminates multicollinearity by transforming correlated features into uncorrelated principal components.\n",
    "- Reduction of computational complexity: Fewer features mean faster model training and inference times.\n",
    "- Enhanced model generalization: By focusing on the most significant sources of variation in the data, PCA can improve the generalization ability of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c874db07",
   "metadata": {},
   "source": [
    "### Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c81a1996",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c8f35d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5da9807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max.transform([[1,5,10,15,20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd51c310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Define the dataset\n",
    "data = np.array([1, 5, 10, 15, 20])\n",
    "\n",
    "# Create an instance of the MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Fit and transform the data using the scaler\n",
    "data_scaled = scaler.fit_transform(data.reshape(-1,1))\n",
    "\n",
    "print(data_scaled.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340b212a",
   "metadata": {},
   "source": [
    "### Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1996659d",
   "metadata": {},
   "source": [
    "To perform feature extraction using PCA (Principal Component Analysis) on the given dataset containing features like height, weight, age, gender, and blood pressure, we follow these steps:\n",
    "\n",
    "1. Standardize the dataset to ensure that all features have a mean of 0 and a standard deviation of 1.\n",
    "2. Apply PCA to the standardized dataset to obtain the principal components.\n",
    "3. Determine the number of principal components to retain based on the explained variance ratio and the desired level of information retention.\n",
    "\n",
    "Let's go through these steps:\n",
    "\n",
    "1. **Standardization**:\n",
    "   Before applying PCA, it's essential to standardize the features to ensure that they all have a comparable scale. This step is crucial because PCA is sensitive to the scale of the features.\n",
    "\n",
    "2. **Apply PCA**:\n",
    "   After standardization, we apply PCA to the standardized dataset to obtain the principal components. Each principal component is a linear combination of the original features, capturing different patterns of variance in the data.\n",
    "\n",
    "3. **Determine the Number of Principal Components**:\n",
    "   We can use the explained variance ratio to decide how many principal components to retain. The explained variance ratio tells us the proportion of variance explained by each principal component. We typically aim to retain enough principal components to explain a significant portion of the total variance in the data, e.g., 95%.\n",
    "\n",
    "   Alternatively, we can use the elbow method or scree plot, which plots the explained variance ratio against the number of principal components. We choose the number of components where the explained variance starts to level off.\n",
    "\n",
    "The choice of how many principal components to retain depends on the specific requirements of the project and the trade-offs between dimensionality reduction and information retention. In general, we want to retain enough principal components to capture the majority of the variance in the data while reducing the dimensionality.\n",
    "\n",
    "For example, if the dataset has 5 features (height, weight, age, gender, blood pressure), we might start by retaining all principal components and then analyze the explained variance ratio or scree plot to determine the optimal number of components to retain. If, for instance, we find that the first three principal components explain more than 95% of the variance, we may choose to retain only these three components.\n",
    "\n",
    "It's essential to strike a balance between dimensionality reduction and retaining enough information to ensure that the reduced-dimensional representation still captures the essential patterns and relationships in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2babe482",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
